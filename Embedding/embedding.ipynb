{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is the first part of building a recommendation system with the Sequential Neural Network.\n",
    "The idea is to generate unique sentences to practice based on the user's error history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning:\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load custom functions from a python file\n",
    "from extra.utility import text_preprocessing, create_unique_word_dict,euclidean,find_similar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('data/sentences_test.csv')\n",
    "texts = [x for x in texts['text']]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We go through all the sentences in the dataset. Preprocess the sentences (tokenize and remove stop words)\n",
    "- We then create pair of words which occur near to each other in a window size equal 2.\n",
    "  - A pair is created for two words say `a` and `b` like `a,b` and `b,a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text\n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_dict = create_unique_word_dict(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting lists to Numpy arrays\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the size of the embedding\n",
    "embed_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a 1 layer Neural network, with an `input dimension = total number of unique words`, which represents our vocabulary. And the `output dimension = size of vocabulary` to get the embeddings for each word of same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network\n",
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "model.add(Dense(units = embed_size, activation = 'linear', input_dim = X.shape[1]))\n",
    "model.add(Dense(units = Y.shape[1], activation = 'softmax'))\n",
    "\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input layer \n",
    "weights = model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the embeddings in 3-dimensions. For this change the `embed_size` to `3` by scrolling 5-6 cells above and running the cells again. Also Uncomment the code cell below to visualize embeddings in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "     coord = embedding_dict.get(word)\n",
    "     ax.scatter(coord[0], coord[1],coord[2])\n",
    "     ax.text(coord[0], coord[1], coord[2],word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean(X[0],Y[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('abandon', embedding_dict, top_n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity between two vectors\n",
    "cosine_similarity(embedding_dict['success'].reshape(1, -1), embedding_dict['bad'].reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Embedding for A level words and sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from the input folder\n",
    "texts_a = pd.read_csv('data/sentences_a.csv')\n",
    "texts_a = [x for x in texts_a['Sentences']]\n",
    "texts_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from the input folder\n",
    "words_a = pd.read_csv('data/unique_words_a.csv')\n",
    "words_a = [x for x in words_a['words']]\n",
    "words_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for sentences with indices as keys\n",
    "sentences_a_dict = {index: value for index, value in enumerate(texts_a)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists_a = []\n",
    "all_sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts_a:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_sentences += text\n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists_a.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists_a.append([word] + [text[(i - w - 1)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for words with indices as keys\n",
    "words_a_dict = {index: value for index, value in enumerate(words_a)}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
